##############################
## by Qin Yu, UCL, Nov 2018
## using Julia 1.0.1
##############################

############################## Starting here:
using LinearAlgebra
using Statistics
using Random
using Plots
using MAT  # Windows only
using JLD  # macOS Friendly
using CSV
using Printf
using DataFrames
push!(LOAD_PATH, ".")
using SuperLearn
gr()


############################## The Boston Housing Dataset:
## Detail see 3.Linear_Regression_Boston_Housing.jl


############################## Import Boston:
#----- (for my Windows 10)
# Import Boston:
matdata = matread("boston.mat")["boston"]
# If MAT package is not available for your version of Julia,
# use JLD to prepare data on another machine:
# save("./boston.jld", "boston", matdata)

#----- (for my macOS)
# use JLD to load the transfored data:
matdata = load("boston.jld")["boston"]
# and have a look at all data of y:
# plot(matdata[:, 14])


############################## 5-fold Cross-validation to Find Best Parameter Pair
# Define a pool of parameters, we want to find the best pair:
ğ›„ = [2.0^i for i in -40:-26]  # regularisation parameter Î³, and
ğ›” = [2.0^i for i in collect(7:.5:13)]  # variance parameter Ïƒ for Gaussian kernel

# Prepare 5-folds data:
nrow, ncol = size(matdata)
nrow_test  = div(nrow, 3)
nrow_train = nrow - nrow_test
Random.seed!(1111)  # this number is again pure magic
permuted_matdata = matdata[randperm(nrow),:]
permuted_matdata_train = permuted_matdata[1:nrow_train,:]
permuted_matdata_test = permuted_matdata[nrow_train+1:end,:]

nrow_5fold  = div(nrow_train, 5)
ğ‘™ = nrow_train - nrow_5fold
ğ‘° = Matrix{Float64}(I, ğ‘™, ğ‘™)  # ğ‘° = Id(l by l)

ğ¾(ğ’™áµ¢, ğ’™â±¼, Ïƒ) = exp(-(norm(ğ’™áµ¢ - ğ’™â±¼)^2)/(2 * Ïƒ^2))  # ğ¾(ğ’™áµ¢, ğ’™â±¼) = ğ‘’^{âˆ¥ğ’™áµ¢-ğ’™â±¼âˆ¥Â²/2ÏƒÂ²}
function fill_ğ‘²(ğ‘², ğ‘¿, Ïƒ)
    nrow_ğ‘², ncol_ğ‘² = size(ğ‘²)
    for i in 1:nrow_ğ‘²
        for j in 1:ncol_ğ‘²
            ğ‘²[i, j] = ğ¾(ğ‘¿[i,:], ğ‘¿[j,:], Ïƒ)  # ğ¾áµ¢â±¼ = ğ¾(ğ’™áµ¢, ğ’™â±¼)
        end
    end
end
get_ğœ¶(ğ‘², Î³, ğ‘°, ğ’š) = inv(ğ‘² + (Î³ * ğ‘™ * ğ‘°)) * ğ’š  # ğ›‚* = (ğ‘² + Î³ğ‘™ğ‘°)â»Â¹â‹…ğ’š
function get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)  # ğ’™â‚ denoted by x
    ğ¾â‚(ğ’™áµ¢, Ïƒ) = ğ¾(ğ’™áµ¢, x', Ïƒ)
    ğ‘¿_train_vectorised = [ğ‘¿_train[i,:]' for i = 1:size(ğ‘¿_train, 1)]
    yÌ‚ = ğœ¶' * ğ¾â‚.(ğ‘¿_train_vectorised, Ïƒ)  # yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€
end

SSE = MSE = zeros(size(ğ›”,1), size(ğ›„,1))
# DO NOT RUN THE TRI-LOOP!!! because otherwise you need to wait.
# If you don't want to wait, call these 2 lines to load my data:
# SSE = load("SSE.jld")["SSE"]
# MSE = load("MSE.jld")["MSE"]
# If you run the 2 lines above, don't /5 later.
for Ïƒ in ğ›”
    Ïƒ_index = findfirst(ğ›” .== Ïƒ)
    for Î³ in ğ›„
        Î³_index = findfirst(ğ›„ .== Î³)
        for i = 0:4
            # get 5-fold data:
            ğ‘¿_test = permuted_matdata_train[(i*nrow_5fold + 1):((i+1) * nrow_5fold), 1:13]
            ğ’š_test = permuted_matdata_train[(i*nrow_5fold + 1):((i+1) * nrow_5fold), 14]

            ğ‘¿_train_1 = permuted_matdata_train[1:(i*nrow_5fold), 1:13]
            ğ‘¿_train_2 = permuted_matdata_train[((1+i)*nrow_5fold+1):end, 1:13]
            ğ‘¿_train = vcat(ğ‘¿_train_1, ğ‘¿_train_2)

            ğ’š_train_1 = permuted_matdata_train[1:(i*nrow_5fold), 14]
            ğ’š_train_2 = permuted_matdata_train[((1+i)*nrow_5fold+1):end, 14]
            ğ’š_train = vcat(ğ’š_train_1, ğ’š_train_2)

            # compute ğ’šÌ‚:
            ğ‘² = zeros(ğ‘™, ğ‘™)
            fill_ğ‘²(ğ‘², ğ‘¿_train, Ïƒ)  # compute kernel matrix, # ğ¾áµ¢â±¼ = ğ¾(ğ’™áµ¢, ğ’™â±¼)

            ğœ¶ = get_ğœ¶(ğ‘², Î³, ğ‘°, ğ’š_train)  # compute ğ›‚*, ğ›‚* = (ğ‘² + Î³ğ‘°)â»Â¹â‹…ğ’š

            ğ‘¿_test_vectorised = [ğ‘¿_test[i,:] for i = 1:size(ğ‘¿_test, 1)]
            get_Å·_(x) = get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)
            yÌ‚ = get_Å·_.(ğ‘¿_test_vectorised)  # compute yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€

            # have a look:
            display(plot(yÌ‚))
            display(plot!(ğ’š_test))

            # compute testing error, MSE:
            sse = sum((ğ’š_test - yÌ‚).^2)  # SSE = ğšºáµ¢(ğ‘¦áµ¢ - Ì‚ğ‘¦áµ¢)Â²
            mse = sse/nrow_5fold  # MSE = SSE/N
            SSE[Ïƒ_index, Î³_index] += sse
            MSE[Ïƒ_index, Î³_index] += mse
        end
    end
end
SSE /= 5
MSE /= 5

# To save it so that I don't need to run again:
save("./data/SSE.jld", "SSE", SSE)
save("./data/MSE.jld", "MSE", MSE)

min_mse, Ïƒ_Î³_indices = findmin(MSE)
Ïƒ_index, Î³_index = Ïƒ_Î³_indices[1], Ïƒ_Î³_indices[2]

# Plot heatmap, take log so that the colourscale makes more sense:
heatmap(log2.(ğ›„), log2.(ğ›”), xticks=-40:-26, xlabel="gamma",
                            yticks=7:.5:13, ylabel="sigma", log.(log.(SSE)))
savefig("4.1.pdf")


############################## !! Comparing Everything !!
# 1 for naive regression
# 2-14 for 1-attribute linear regression
# 15 for all-attribute linear regression
jl4_ğ„ = zeros(16, 2)
jl4_ğ›” = zeros(16, 2)

# You can skip to last part by loading these:
# jl4_ğ„ = load("jl4_E.jld")["E"]
# jl4_ğ›” = load("jl4_s.jld")["s"]
############################## ADDING: Kernel Ridge Regression
# Run on whole training and testing sets:
Ïƒ = ğ›”[Ïƒ_index]
Î³ = ğ›„[Î³_index]

ğ‘¿_test  = permuted_matdata_test[:, 1:13]
ğ‘¿_train = permuted_matdata_train[:, 1:13]
ğ’š_test  = permuted_matdata_test[:, 14]
ğ’š_train = permuted_matdata_train[:, 14]

ğ‘™ = nrow_train
ğ‘° = Matrix{Float64}(I, ğ‘™, ğ‘™)  # ğ‘° = Id(l x l)

ğ‘² = zeros(ğ‘™, ğ‘™)
fill_ğ‘²(ğ‘², ğ‘¿_train, Ïƒ)  # compute kernel matrix, # ğ¾áµ¢â±¼ = ğ¾(ğ’™áµ¢, ğ’™â±¼)

ğœ¶ = get_ğœ¶(ğ‘², Î³, ğ‘°, ğ’š_train)  # compute ğ›‚*, ğ›‚* = (ğ‘² + Î³ğ‘°)â»Â¹â‹…ğ’š

ğ‘¿_train_vectorised = [ğ‘¿_train[i,:] for i = 1:size(ğ‘¿_train, 1)]
get_Å·_(x) = get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)
yÌ‚_train = get_Å·_.(ğ‘¿_train_vectorised)  # compute yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€

ğ‘¿_test_vectorised = [ğ‘¿_test[i,:] for i = 1:size(ğ‘¿_test, 1)]
get_Å·_(x) = get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)
yÌ‚_test = get_Å·_.(ğ‘¿_test_vectorised)  # compute yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€

sse_train = sum((ğ’š_train - yÌ‚_train).^2)  # SSE = ğšºáµ¢(ğ‘¦áµ¢ - Ì‚ğ‘¦áµ¢)Â²
mse_train = sse_train/nrow_train  # MSE = SSE/N

sse_test = sum((ğ’š_test - yÌ‚_test).^2)  # SSE = ğšºáµ¢(ğ‘¦áµ¢ - Ì‚ğ‘¦áµ¢)Â²
mse_test = sse_test/nrow_test  # MSE = SSE/N

sse20_test  = zeros(20)
sse20_train = zeros(20)
for i = 1:20
    permuted_matdata = matdata[randperm(nrow),:]
    permuted_matdata_train = permuted_matdata[1:nrow_train,:]
    permuted_matdata_test = permuted_matdata[nrow_train+1:end,:]

    Ïƒ = ğ›”[Ïƒ_index]
    Î³ = ğ›„[Î³_index]

    ğ‘¿_test  = permuted_matdata_test[:, 1:13]
    ğ‘¿_train = permuted_matdata_train[:, 1:13]
    ğ’š_test  = permuted_matdata_test[:, 14]
    ğ’š_train = permuted_matdata_train[:, 14]

    ğ‘™ = nrow_train
    ğ‘° = Matrix{Float64}(I, ğ‘™, ğ‘™)  # ğ‘° = Id(l x l)

    ğ‘² = zeros(ğ‘™, ğ‘™)
    fill_ğ‘²(ğ‘², ğ‘¿_train, Ïƒ)  # compute kernel matrix, # ğ¾áµ¢â±¼ = ğ¾(ğ’™áµ¢, ğ’™â±¼)

    ğœ¶ = get_ğœ¶(ğ‘², Î³, ğ‘°, ğ’š_train)  # compute ğ›‚*, ğ›‚* = (ğ‘² + Î³ğ‘°)â»Â¹â‹…ğ’š

    ğ‘¿_train_vectorised = [ğ‘¿_train[i,:] for i = 1:size(ğ‘¿_train, 1)]
    get_Å·_(x) = get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)
    yÌ‚_train = get_Å·_.(ğ‘¿_train_vectorised)  # compute yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€

    ğ‘¿_test_vectorised = [ğ‘¿_test[i,:] for i = 1:size(ğ‘¿_test, 1)]
    get_Å·_(x) = get_yÌ‚(ğœ¶, x, Ïƒ, Î³, ğ‘¿_train)
    yÌ‚_test = get_Å·_.(ğ‘¿_test_vectorised)  # compute yÌ‚ = ğœ¶áµ€â‹…(ğ¾â‚â‚, ğ¾â‚‚â‚, ..., ğ¾â‚—â‚)áµ€

    sse20_train[i] = sum((ğ’š_train - yÌ‚_train).^2)  # SSE = ğšºáµ¢(ğ‘¦áµ¢ - Ì‚ğ‘¦áµ¢)Â²
    #mse_train = sse_train/nrow_train  # MSE = SSE/N

    sse20_test[i] = sum((ğ’š_test - yÌ‚_test).^2)  # SSE = ğšºáµ¢(ğ‘¦áµ¢ - Ì‚ğ‘¦áµ¢)Â²
    #mse_test = sse_test/nrow_test  # MSE = SSE/N
end

mse20_train = sse20_train/nrow_train
mse20_test  = sse20_test/nrow_test
plot(mse20_train, lab="training error")
plot!(mse20_test, lab="testing error")
savefig("4.2.pdf")

# xâ‚, ..., xâ‚™ are n independent obeservations from a population
# that has mean Î¼ and variance Ïƒ, then the variance of Î£xáµ¢ is nÏƒÂ²
# and the variance of xÌ„ = Î£xáµ¢/n is ÏƒÂ²/n
jl4_ğ„[16, 1] = ğ„mse_train = mean(mse20_train)
jl4_ğ„[16, 2] = ğ„mse_test  = mean(mse20_test)

jl4_ğ›”[16, 1] = se_train = std(mse20_train)
jl4_ğ›”[16, 2] = se_test  = std(mse20_test)


############################## ADDING: Naive Regression
# 20 runs of randomly splitted datasets:
ones_test  = ones(nrow_test)
ones_train = ones(nrow_train)
plot(matdata[:, 14])
sum_all_20_te  = zeros(20)  # no longer sum
sum_all_20_tse = zeros(20)
for i = 1:20
    # Obtain randomly splitted ğ’š_test/training:
    permuted_matdata = matdata[randperm(nrow),:]
    ğ’š_train = permuted_matdata[nrow_test+1:nrow, 14]
    ğ’š_test  = permuted_matdata[1:nrow_test, 14]

    # Plot each run:
    trl(x_test) = trained_regression_line(x_test, ones_train, ğ’š_train, 1)
    plot!(trl, 1, nrow)
    #println("done")

    # Obtain MSEs:
    sum_all_20_te[i]  = training_error_k_dim_basis(ones_train, ğ’š_train, 1)
    sum_all_20_tse[i] = test_error_k_dim_basis(ones_test, ğ’š_test, ones_train, ğ’š_train, 1)
end

# Show the result of plot loop by current():
current()

jl4_ğ„[1, 1] = ğ„mse_naive_train = mean(sum_all_20_te)
jl4_ğ„[1, 2] = ğ„mse_naive_test  = mean(sum_all_20_tse)

jl4_ğ›”[1, 1] = se_naive_train = std(sum_all_20_te)
jl4_ğ›”[1, 2] = se_naive_test  = std(sum_all_20_tse)


############################## ADDING: Linear Regression (attribute i)
# Using SuperLearn without basis function (but with integrated [ ,1]):
Core.eval(SuperLearn, :(TRANS_BASIS = false))

# the ğ‘–th element is for the ğ‘–th ğ‘¥:
sum_all_20_te  = zeros(14, 20)  # This is called sum for my historical reason
sum_all_20_tse = zeros(14, 20)
for j = 1:20
    permuted_matdata = matdata[randperm(nrow),:]
    for i = 1:13
        ğ’™_test  = permuted_matdata[1:nrow_test, i]
        ğ’™_train = permuted_matdata[nrow_test+1:nrow, i]
        ğ’š_test  = permuted_matdata[1:nrow_test, 14]
        ğ’š_train = permuted_matdata[nrow_test+1:nrow, 14]
        scatter(ğ’™_train, ğ’š_train)
        trl(x_test) = trained_regression_line(x_test, ğ’™_train, ğ’š_train, nothing)
        display(plot!(trl, minimum(ğ’™_train), maximum(ğ’™_train)))

        sum_all_20_te[i, j]  = training_error_k_dim_basis(ğ’™_train, ğ’š_train, nothing)
        sum_all_20_tse[i, j] = test_error_k_dim_basis(ğ’™_test, ğ’š_test, ğ’™_train, ğ’š_train, nothing)
    end
end

jl4_ğ„[2:14, 1] = ğ„mse_xi_train = [mean(sum_all_20_te[i,:])  for i in 1:13]
jl4_ğ„[2:14, 2] = ğ„mse_xi_test  = [mean(sum_all_20_tse[i,:]) for i in 1:13]

jl4_ğ›”[2:14, 1] = se_xi_train = [std(sum_all_20_te[i,:])  for i in 1:13]
jl4_ğ›”[2:14, 2] = se_xi_test  = [std(sum_all_20_tse[i,:]) for i in 1:13]


############################## ADDING: Linear Regression (all attribute)
plot(matdata[:, 14])
X_test  = matdata[1:nrow_test, 1:13]
X_train = matdata[nrow_test+1:nrow, 1:13]
ğ’š_test  = matdata[1:nrow_test, 14]
ğ’š_train = matdata[nrow_test+1:nrow, 14]
trl(X_test) = trained_regression_line_M(X_test, X_train, ğ’š_train, nothing)
scatter!(nrow_test+1:nrow, trl(X_train))  # current() outside loop, display() inside!!

sorted_matdata = sort_matrix_by_jth_col(matdata, 14)
for j = 1:20
    permuted_matdata = matdata[randperm(nrow),:]
    X_test  = permuted_matdata[1:nrow_test, 1:13]
    X_train = permuted_matdata[nrow_test+1:nrow, 1:13]
    ğ’š_test  = permuted_matdata[1:nrow_test, 14]
    ğ’š_train = permuted_matdata[nrow_test+1:nrow, 14]

    sum_all_20_te[14, j]  = training_error_k_dim_basis(X_train, ğ’š_train, nothing)
    sum_all_20_tse[14, j] = test_error_k_dim_basis(X_test, ğ’š_test, X_train, ğ’š_train, nothing)
end

jl4_ğ„[15, 1] = ğ„mse_xall_train = mean(sum_all_20_te[14,:])
jl4_ğ„[15, 2] = ğ„mse_xall_test  = mean(sum_all_20_tse[14,:])

jl4_ğ›”[15, 1] = se_xall_train = std(sum_all_20_te[14,:])
jl4_ğ›”[15, 2] = se_xall_test  = std(sum_all_20_tse[14,:])

jl4_ğ„
jl4_ğ›”
save("./data/jl4_E.jld", "E", jl4_ğ„)
save("./data/jl4_s.jld", "s", jl4_ğ›”)

# Print Table:
# jl4_ğ„ = load("jl4_E.jld")["E"]
# jl4_ğ›” = load("jl4_s.jld")["s"]
jl4_ğ„_ğ›” = hcat(jl4_ğ„[:,1], jl4_ğ›”[:,1], jl4_ğ„[:,2], jl4_ğ›”[:,2])
jl4_ğ„_ğ›”_rd = round.(jl4_ğ„_ğ›”, digits=4)
jl4_ğ„_ğ›”_str = (x -> @sprintf("%.2f", x)).(jl4_ğ„_ğ›”_rd)
jl4_ğ„_str = (x -> @sprintf("%.2f", x)).(jl4_ğ„)
jl4_ğ›”_str = (x -> @sprintf("%.2f", x)).(jl4_ğ›”)
jl4 = jl4_ğ„_str .* " Â± " .* jl4_ğ›”_str

q5_DataFrame = convert(DataFrame, jl4)
names!(q5_DataFrame, [:E, :sigma])
q5_DataFrame[:Regression] = vcat(["Naive"], ["x$i" for i in 1:13], ["x"], ["KRR"])
q5_DataFrame = q5_DataFrame[[:Regression, :E, :sigma]]

print(q5_DataFrame)
io = open("./data/final_compare.txt", "w")
print(io, q5_DataFrame)
close(io)

# 16Ã—3 DataFrame
# â”‚ Row â”‚ Regression â”‚ E            â”‚ sigma         â”‚
# â”‚     â”‚ String     â”‚ String       â”‚ String        â”‚
# â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 1   â”‚ Naive      â”‚ 82.03 Â± 6.00 â”‚ 89.53 Â± 12.30 â”‚
# â”‚ 2   â”‚ x1         â”‚ 72.15 Â± 4.32 â”‚ 71.45 Â± 8.54  â”‚
# â”‚ 3   â”‚ x2         â”‚ 74.20 Â± 4.12 â”‚ 72.22 Â± 8.27  â”‚
# â”‚ 4   â”‚ x3         â”‚ 65.28 Â± 3.77 â”‚ 63.75 Â± 7.48  â”‚
# â”‚ 5   â”‚ x4         â”‚ 82.31 Â± 4.31 â”‚ 81.25 Â± 8.66  â”‚
# â”‚ 6   â”‚ x5         â”‚ 69.85 Â± 3.73 â”‚ 67.59 Â± 7.25  â”‚
# â”‚ 7   â”‚ x6         â”‚ 43.50 Â± 4.35 â”‚ 44.30 Â± 9.29  â”‚
# â”‚ 8   â”‚ x7         â”‚ 73.26 Â± 3.83 â”‚ 71.03 Â± 7.53  â”‚
# â”‚ 9   â”‚ x8         â”‚ 80.05 Â± 4.53 â”‚ 77.61 Â± 9.00  â”‚
# â”‚ 10  â”‚ x9         â”‚ 72.69 Â± 4.61 â”‚ 71.30 Â± 9.20  â”‚
# â”‚ 11  â”‚ x10        â”‚ 66.64 Â± 4.22 â”‚ 64.70 Â± 8.38  â”‚
# â”‚ 12  â”‚ x11        â”‚ 62.96 Â± 4.27 â”‚ 62.41 Â± 8.58  â”‚
# â”‚ 13  â”‚ x12        â”‚ 75.28 Â± 4.30 â”‚ 74.74 Â± 8.49  â”‚
# â”‚ 14  â”‚ x13        â”‚ 38.70 Â± 2.03 â”‚ 38.32 Â± 4.03  â”‚
# â”‚ 15  â”‚ x          â”‚ 22.03 Â± 1.57 â”‚ 23.13 Â± 3.53  â”‚
# â”‚ 16  â”‚ KRR        â”‚ 8.13 Â± 0.76  â”‚ 12.90 Â± 2.16  â”‚
