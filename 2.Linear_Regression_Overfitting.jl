##############################
## by Qin Yu, Nov 2018
## using Julia 1.0.1
##############################

# If you have not installed these packages:
# using Pkg # Julia 1.0.1 only
# Pkg.add("GR")
# Pkg.add("Plots")
# Pkg.add("PyPlot")
# Pkg.add("Distributions")

# I will not include Pkg.add info in future .jl files,
# add it if you can't be "using" any.

############################## Starting here:
using LinearAlgebra
using Plots
using Random
using Distributions
# Load my own package:
push!(LOAD_PATH, ".")
using SuperLearn

############################## Change Basis, Run:
# Core.eval(SuperLearn, :(POLY_OR_SINE = "poly"))  # Change to Polynomial Basis (default)
# Core.eval(SuperLearn, :(POLY_OR_SINE = "sine"))  # Change to Sine Basis


############################## Another Simple Dataset:
# random function ğ‘”_Ïƒ(ğ‘¥) := sinÂ²(2Ï€ğ‘¥) + Ïµ, where random var Ïµ ~ ğ(0, ÏƒÂ²)
# sample size n = 30, iid random variable ğ‘‹áµ¢ ~ ğ”(0, 1)
# srand(123)  # Julia 0.6.4
Random.seed!(777)  # Julia 1.0.1

# Define a distribution that we are sampling from, and sample:
ğ‘¥áµ¢ = Uniform(0, 1)
ğ’™ = rand(ğ‘¥áµ¢, 30)
sort!(ğ’™)

# Plot the function sinÂ²(2Ï€x) in the range 0 â‰¤ x â‰¤ 1
f(x) = (sin(2 * Ï€ * x))^2
plot(f, 0, 1, lab="sinÂ²(2 pi x)")

# with the points of the above data set superimposed:
g(Ïƒ, x) = f(x) + rand(Normal(0, Ïƒ), 1)[1]  # add noise when we sample
gâ‚€â‚€â‚‡(x) = g(0.07, x)
ğ’š = vcat(gâ‚€â‚€â‚‡.(ğ’™)...)
scatter!(ğ’™, ğ’š, lab="S")

savefig("2.1.pdf")

############################## Train 20 regressions:
# I am using trained_regression_line() from my own package,
# which is extracted from 1.Linear_Regression_Basis_Function.jl
regression_curves = [x -> trained_regression_line(x, ğ’™, ğ’š, i) for i in 1:20]

# Plot selected learned linear regression curves (every third):
for i = 1:3:size(regression_curves, 1)
    display(plot(regression_curves[i], 0, 1, lab="k = $i"))
    display(scatter!(ğ’™, ğ’š, lab="S"))
    savefig("2.2-$i.pdf")
end


############################## Training Errors for Different Sizes of Basis:
te_k(k) = training_error_k_dim_basis(ğ’™, ğ’š, k)
te_k_1to20 = te_k.(1:20)
log_te_k_1to20 = log.(te_k_1to20)
#plot(1:20, te_k_1to20)
plot(1:20, log_te_k_1to20, xlabel="k", xticks=0:20, ylabel="log training MSE", lab="log(te(k, S))")

############################## Testing Error with Testing Set of Size 1000:
ğ’™_test = rand(ğ‘¥áµ¢, 1000)
sort!(ğ’™_test)
ğ’š_test = vcat(gâ‚€â‚€â‚‡.(ğ’™_test)...)
scatter(ğ’™_test, ğ’š_test)

tse_k(k) = test_error_k_dim_basis(ğ’™_test, ğ’š_test, ğ’™, ğ’š, k)
tse_k_1to20 = tse_k.(1:20)
log_tse_k_1to20 = log.(tse_k_1to20)
#plot!(1:20, tse_k_1to20)
plot(1:20, log_te_k_1to20, xlabel="k", xticks=0:20, ylabel="log MSE", lab="log(te(k, S))")
plot!(1:20, log_tse_k_1to20, lab="log(tse(k, S, T))")
# tse_k(k) = test_error_k_dim_basis(ğ’™, ğ’š, ğ’™, ğ’š, k)  # Testing if this func's correctness
savefig("2.3.pdf")

############################## Obtain Previous Result 100 Times and Get Log-Average:
sum_all_100_te = fill(0, 20)
sum_all_100_tse = fill(0, 20)
for n = 1:100
    ğ’™ = rand(ğ‘¥áµ¢, 30)  # increasing the dimension of training set REDUCEs the error
    sort!(ğ’™)
    ğ’š = vcat(gâ‚€â‚€â‚‡.(ğ’™)...)
    te_k(k) = training_error_k_dim_basis(ğ’™, ğ’š, k)
    te_k_1to20 = te_k.(1:20)
    global sum_all_100_te += te_k.(1:20)

    ğ’™_test = rand(ğ‘¥áµ¢, 1000)
    sort!(ğ’™_test)
    ğ’š_test = vcat(gâ‚€â‚€â‚‡.(ğ’™_test)...)
    tse_k(k) = test_error_k_dim_basis(ğ’™_test, ğ’š_test, ğ’™, ğ’š, k)
    tse_k_1to20 = tse_k.(1:20)
    global sum_all_100_tse += tse_k.(1:20)
end

avg_all_100_te = sum_all_100_te / 100
avg_all_100_tse = sum_all_100_tse / 100
log_all_100_te = log.(avg_all_100_te)
log_all_100_tse = log.(avg_all_100_tse)
plot(1:20, log_all_100_te, xlabel="k", xticks=0:20, ylabel="log average MSE", lab="log(avg(te(k, S)))")
plot!(1:20, log_all_100_tse, lab="log(avg(tse(k, S, T)))")  # run this section again, the larger k tail varies
savefig("2.4.pdf")


############################## To Change Polynomial Basis to Sine Basis, Run:
Core.eval(SuperLearn, :(POLY_OR_SINE = "sine"))
# Changed the basis, run everything again.
